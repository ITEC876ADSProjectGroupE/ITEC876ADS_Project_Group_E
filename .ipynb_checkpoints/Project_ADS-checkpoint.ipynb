{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "import string\n",
    "import pandas as pd\n",
    "from stackapi import StackAPI\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Custom functions created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(data, name):\n",
    "    \n",
    "    '''\n",
    "    Function that creates a CSV file from the dictionary data object returned by the API.\n",
    "    \n",
    "    Input \n",
    "           : data -> data from API -> type : dictionary\n",
    "           : name -> name of the CSV file -> type : string\n",
    "    \n",
    "    Output : Returns nothing\n",
    "    '''\n",
    "    \n",
    "    # Extracting the \"items\" values from the dictionary data storing into data_items\n",
    "    data_items = data[\"items\"]\n",
    "    \n",
    "    # Creating an empty dataframe\n",
    "    data_frame = pd.DataFrame()\n",
    "    \n",
    "    # Creating empty lists for questions as well as for tags of the questions\n",
    "    question_list = []\n",
    "    tags_list = []\n",
    "\n",
    "    # Looping through data items\n",
    "    for i in data_items:\n",
    "        \n",
    "        # Extracting title from the iterator i of data items, appending it to question_list\n",
    "        question_list.append(i[\"title\"])\n",
    "        \n",
    "        # Extracting tags from the iterator i of data items\n",
    "        # Formating the tags for e.g \"__label__TAG1 __label__TAG2\" (Required by fasttext)\n",
    "        # Appending it to tags_list\n",
    "        tags_list.append(\"\".join([\"__label__\" + str(i) + \" \" for i in i[\"tags\"]]))\n",
    "    \n",
    "    # Creating a column named \"Question\" using question_list\n",
    "    data_frame[\"Question\"] = question_list\n",
    "    \n",
    "    # Creating a column named \"Labels\" using tags_list\n",
    "    data_frame[\"Labels\"] = tags_list\n",
    "    \n",
    "    # Saving the data_frame to csv file\n",
    "    data_frame.to_csv(name + \".csv\", header = True)\n",
    "    \n",
    "def dataframe_to_text(df, clean):\n",
    "    \n",
    "    '''\n",
    "    Function that creates a text file from the pandas dataframe.\n",
    "    \n",
    "    Input  \n",
    "           : df -> type : pandas dataframe\n",
    "           : clean -> whether to clean data or not -> type : boolean\n",
    "            \n",
    "    Output : Returns nothing\n",
    "    '''\n",
    "    \n",
    "    # Creating an empty string\n",
    "    final_string = \"\"\n",
    "    \n",
    "    filename = \"final_data.txt\"\n",
    "    \n",
    "    if clean:\n",
    "        filename = \"final_data_processed.txt\"\n",
    "    \n",
    "    # Opening(Creating if not exist) a text file named \"final_data\" in writing mode with encoding utf-8\n",
    "    with open(filename, 'w', encoding = 'utf-8') as file:\n",
    "        \n",
    "        # Iterating through df(dataframe) rows\n",
    "        for index, row in df.iterrows():\n",
    "            \n",
    "            # Storing the question\n",
    "            question = row[\"Question\"]\n",
    "            \n",
    "            if clean:\n",
    "                # When data is gathered from APIs there are generally HTML escape characters in the data\n",
    "                # Using html.unenscape() to convert HTML escape characters into string in lowercase\n",
    "                question = html.unescape(question).lower()\n",
    "\n",
    "                # Cleaning Latex formulas\n",
    "                question = re.sub(r'\\$[\\s\\S]*\\$',\"\", question).strip()\n",
    "\n",
    "                # Keeping the '-' because it is used to join words for example part-time\n",
    "                question = re.sub(r'[^A-Za-z0-9\\-\\s]+', \"\", question).strip()\n",
    "            \n",
    "            # Appending labels(lowercase) and question to string\n",
    "            final_string += (row[\"Labels\"] + question)\n",
    "            \n",
    "            # Appending new line\n",
    "            final_string += '\\n'\n",
    "        \n",
    "        # Writing the entire string to file\n",
    "        file.write(final_string.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Setting parameters for API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Authentication (OAuth call)\n",
    "c_key = \"qiEDB0WIq3e3MM7LRZB*3Q((\"\n",
    "c_secret = \"L7Pv1pOb2VJxDvBy9qH38A((\" \n",
    "\n",
    "# Configuring the APIs\n",
    "## INFORMATION SECURITY\n",
    "SITE_IS = StackAPI('security', key = c_key, client_secret = c_secret)\n",
    "\n",
    "## STATISTICS\n",
    "SITE_ST = StackAPI('stats', key = c_key, client_secret = c_secret)\n",
    "\n",
    "## PHYSICS\n",
    "SITE_PH = StackAPI('physics', key = c_key, client_secret = c_secret)\n",
    "\n",
    "# TOTAL QUESTION = page_sizes * max_pages = 50,000\n",
    "## INFORMATION SECURITY\n",
    "SITE_IS.page_sizes = 100\n",
    "SITE_IS.max_pages = 500\n",
    "\n",
    "## STATISTICS\n",
    "SITE_ST.page_sizes = 100\n",
    "SITE_ST.max_pages = 500\n",
    "\n",
    "## PHYSICS\n",
    "SITE_PH.page_sizes = 100\n",
    "SITE_PH.max_pages = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Calling the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questions for INFORMATION SECURITY\n",
    "data_IS = SITE_IS.fetch(\"questions\")\n",
    "\n",
    "# Questions for STATISTICS\n",
    "data_ST = SITE_ST.fetch(\"questions\")\n",
    "\n",
    "# Questions for PHYSICS\n",
    "data_PH = SITE_PH.fetch(\"questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Creating dataframe from the object returned by StackAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For more information about the function \"create_dataframe\" go to section 1. Custom Functions Created\n",
    "create_dataframe(data_IS, \"IS\")\n",
    "create_dataframe(data_ST, \"STATS\")\n",
    "create_dataframe(data_PH, \"PHYSICS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Merging all the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from csv files\n",
    "df_IS = pd.read_csv(\"IS.csv\", index_col = 0)\n",
    "df_ST = pd.read_csv(\"STATS.csv\", index_col = 0)\n",
    "df_PH = pd.read_csv(\"PHYSICS.csv\", index_col = 0)\n",
    "\n",
    "# Concatinating them into one big dataframe\n",
    "df_list = [df_IS, df_PH, df_ST]\n",
    "df_final = pd.concat(df_list)\n",
    "\n",
    "# Shuffling the rows for randomness\n",
    "df_final = shuffle(df_final)\n",
    "\n",
    "# Removing the index column created by shuffle function \n",
    "df_final.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# Saving the dataframe to csv file\n",
    "df_final.to_csv(\"final.csv\", header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Dataframe to Text file (Required by fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading a csv file\n",
    "df_final = pd.read_csv(\"final.csv\")\n",
    "\n",
    "# Creating a text file from dataframe and also cleaning the data\n",
    "# For more information about the function \"dataframe_to_text\" go to section 1. Custom Functions Created\n",
    "\n",
    "# WITHOUT CLEANING\n",
    "dataframe_to_text(df = df_final, clean = False)\n",
    "\n",
    "# WITH CLEANING\n",
    "dataframe_to_text(df = df_final, clean = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Creating Train(70%), Validation(20%) and Test(10%) Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training = 150,000 * 0.70 = 105000 observations\n",
    "\n",
    "Validation = 150,000 * 0.20 = 30000 observations\n",
    "\n",
    "Testing = 150,000 * 0.10 = 15000 observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.1 Without cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head -n 105000 final_data.txt > data.train\n",
    "tail -n 15000 final_data.txt > data.test\n",
    "tail -n 45000 final_data.txt | head -n 30000 > data.valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.2 With cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head -n 105000 final_data_processed.txt > data_processed.train\n",
    "tail -n 15000 final_data_processed.txt > data_processed.test\n",
    "tail -n 45000 final_data_processed.txt | head -n 30000 > data_processed.valid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
